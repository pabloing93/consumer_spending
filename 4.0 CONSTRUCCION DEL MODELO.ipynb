{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRH2u-dX7WzM"
      },
      "source": [
        "## **4. Construcción de Modelos** (entrenamiento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "# Si se usa colab hay que instalar catboost\n",
        "try:\n",
        "    import catboost as cat\n",
        "except Exception:\n",
        "    !pip install catboost\n",
        "finally:\n",
        "    import catboost as cat\n",
        "\n",
        "from datetime import date\n",
        "from joblib import dump, load\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
        "\n",
        "import warnings; warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.max_rows', 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CARGA DE DATOS POST ANALISIS\n",
        "df = pd.read_csv('datos_luego_de_eda.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def xgbclass(seed=42, data=data):\n",
        "        # Clasificador\n",
        "        clf_xgb = xgb.XGBClassifier(objective='binary:logistic', random_state=seed, n_jobs=-1)\n",
        "\n",
        "        # Otra forma de hacerlo, con clasificador y regresor usando xgboost\n",
        "        X = data.drop('transactionRevenue', axis=1)\n",
        "        y = data['transactionRevenue'].apply(lambda x: 1 if x > 0 else 0)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "        # Creamos un random search para encontrar los mejores hiperparámetros\n",
        "        params = {\n",
        "                'booster': ['gbtree', 'gblinear', 'dart'], # Tipo de modelo\n",
        "                'n_estimators': [100, 200, 400], # Número de árboles\n",
        "                'learning_rate': [0.1, 0.06, 0.03, 0.01],  # Tasa de aprendizaje\n",
        "                }\n",
        "\n",
        "        # Obtenemos los mejores hiperparámetros\n",
        "        search = RandomizedSearchCV(\n",
        "                estimator=clf_xgb, \n",
        "                param_distributions=params, \n",
        "                scoring='accuracy',\n",
        "                cv=5, \n",
        "                n_iter=5)\n",
        "        search.fit(X_train, y_train)\n",
        "        best_params = search.best_params_\n",
        "\n",
        "        # Entrenamos el clasificador\n",
        "        clf_xgb = xgb.XGBClassifier(**best_params, random_state=seed, n_jobs=-1)\n",
        "        clf_xgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=0)\n",
        "\n",
        "        # Predecimos con el clasificador\n",
        "        y_pred_cls_xgb = clf_xgb.predict(X_test)\n",
        "\n",
        "        # Score del clasificador\n",
        "        print(f'Precisión del clasificador: {accuracy_score(y_test, y_pred_cls_xgb):.3f}')\n",
        "\n",
        "        # Calibración más Precisa de las Probabilidades\n",
        "        cal_clf_xgb = CalibratedClassifierCV(clf_xgb, method='sigmoid')\n",
        "        cal_clf_xgb.fit(X_train, y_train)\n",
        "\n",
        "        # Probabilidades de Clase Calibradas\n",
        "        calibrated_y_pred = cal_clf_xgb.predict(X_test)\n",
        "\n",
        "        # Medir el Rendimiento\n",
        "        print(f'Precisión del clasificador calibrado: {accuracy_score(y_test, calibrated_y_pred):.3f}')\n",
        "\n",
        "        if accuracy_score(y_test, y_pred_cls_xgb) < accuracy_score(y_test, calibrated_y_pred):\n",
        "            clf_xgb = cal_clf_xgb\n",
        "\n",
        "        return clf_xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_xgb = xgbclass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def xgbreg(seed=42, data=data):\n",
        "        # Regresor\n",
        "        reg_xgb = xgb.XGBRegressor(objective='reg:squarederror', random_state=seed)\n",
        "\n",
        "        # Agregamos la columna 'class' y dividimos en entrenamiento y prueba\n",
        "        data['class'] = clf_xgb.predict(data.drop(columns=['transactionRevenue']))\n",
        "        X = data.drop(columns=['transactionRevenue'])\n",
        "        y = data['transactionRevenue']\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "        # Creamos un random search para encontrar los mejores hiperparámetros\n",
        "        params = {\n",
        "                'booster': ['gbtree', 'gblinear', 'dart'], # Tipo de modelo\n",
        "                'n_estimators': [50, 150, 500], # Número de árboles\n",
        "                'learning_rate': [0.1, 0.06, 0.03, 0.01],  # Tasa de aprendizaje\n",
        "                }\n",
        "\n",
        "        # Obtenemos los mejores hiperparámetros\n",
        "        search = RandomizedSearchCV(\n",
        "                estimator=reg_xgb, \n",
        "                param_distributions=params, \n",
        "                scoring='neg_mean_squared_error', \n",
        "                cv=5, \n",
        "                n_iter=5)\n",
        "        search.fit(X_train, y_train)\n",
        "        best_params = search.best_params_\n",
        "\n",
        "        # Entrenamos el regresor agregando los mejores hiperparámetros\n",
        "        reg_xgb = xgb.XGBRegressor(**best_params, objective='reg:squarederror', random_state=seed)\n",
        "        reg_xgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=10, verbose=0)\n",
        "\n",
        "        # Predecimos con el regresor\n",
        "        y_pred_reg_xgb = reg_xgb.predict(X_test)\n",
        "\n",
        "        # Convertimos todas las predicciones menores a 0.01 en 0\n",
        "        y_pred_reg_xgb = np.where(y_pred_reg_xgb < 0.01, 0, y_pred_reg_xgb)\n",
        "\n",
        "        # Score del regresor para rmse y r2\n",
        "        print(f'Raiz del error para el regresor: {np.sqrt(mean_squared_error(y_test, y_pred_reg_xgb)):.3f}')\n",
        "\n",
        "        return reg_xgb, y_pred_reg_xgb, np.sqrt(mean_squared_error(y_test, y_pred_reg_xgb)), r2_score(y_test, y_pred_reg_xgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rmse = 21\n",
        "while rmse >= 16:\n",
        "    data = df.copy()\n",
        "    seed = np.random.randint(1, 100000)\n",
        "    reg_xgb, reg_xgb_pred, xgb_rmse, xgb_r2 = xgbreg(seed, data)\n",
        "    if rmse < 16:\n",
        "        print('El regresor logro el objetivo:')\n",
        "        print(f'El valor de la semilla es: {seed}')\n",
        "        print(f'Los hiperparámetros son: {reg_xgb.get_params()}')\n",
        "        print(f'El rmse es: {xgb_rmse}')\n",
        "        print(f'El r2 es: {xgb_r2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> mejores hiperparametros encontrados: ``{'objective': 'reg:squarederror', 'base_score': None, 'booster': 'gblinear', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 500, 'n_jobs': None, 'num_parallel_tree': None, 'predictor': None, 'random_state': 31510, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Guardamos el modelo XGBoost\n",
        "# dump(clf_xgb, 'clasificador_xgb.joblib')\n",
        "# dump(reg_xgb, 'regresor_xgb.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lbgclass(seed=42, data=data):\n",
        "        # Clasificador\n",
        "        clf_lgb = lgb.LGBMClassifier(random_state=seed, n_jobs=-1, verbosity=-1)\n",
        "\n",
        "        # Dividimos en entrenamiento y prueba\n",
        "        X = data.drop('transactionRevenue', axis=1)\n",
        "        y = data['transactionRevenue'].apply(lambda x: 1 if x > 0 else 0)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "        # Creamos un random search para encontrar los mejores hiperparámetros\n",
        "        params = {\n",
        "                'boosting_type': ['gbdt', 'dart', 'goss'], # Tipo de modelo\n",
        "                'num_boost_round': [100, 400, 800], # Número de árboles\n",
        "                'max_depth': [2, 3, 5], # Profundidad máxima\n",
        "                'learning_rate': [0.1, 0.06, 0.03, 0.01],  # Tasa de aprendizaje\n",
        "                }\n",
        "\n",
        "        # Obtenemos los mejores hiperparámetros\n",
        "        search = RandomizedSearchCV(\n",
        "                estimator=clf_lgb, \n",
        "                param_distributions=params, \n",
        "                scoring='accuracy', \n",
        "                cv=5, \n",
        "                n_iter=5)\n",
        "        search.fit(X_train, y_train)\n",
        "        best_params = search.best_params_\n",
        "\n",
        "        # Entrenamos el clasificador\n",
        "        clf_lgb = lgb.LGBMClassifier(**best_params, random_state=seed, n_jobs=-1, verbosity=-1)\n",
        "        clf_lgb.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
        "\n",
        "        # Predecimos con el clasificador\n",
        "        y_pred_cls_lgb = clf_lgb.predict(X_test)\n",
        "\n",
        "        # Score del clasificador\n",
        "        print(f'Precisión del clasificador: {accuracy_score(y_test, y_pred_cls_lgb):.3f}')\n",
        "\n",
        "        # Calibración más Precisa de las Probabilidades\n",
        "        cal_clf_lgb = CalibratedClassifierCV(clf_lgb, method='sigmoid')\n",
        "        cal_clf_lgb.fit(X_train, y_train)\n",
        "\n",
        "        # Probabilidades de Clase Calibradas\n",
        "        calibrated_y_pred = cal_clf_lgb.predict(X_test)\n",
        "\n",
        "        # Medir el Rendimiento\n",
        "        print(f'Precisión del clasificador calibrado: {accuracy_score(y_test, calibrated_y_pred):.3f}')\n",
        "\n",
        "        if accuracy_score(y_test, y_pred_cls_lgb) < accuracy_score(y_test, calibrated_y_pred):\n",
        "            clf_lgb = cal_clf_lgb\n",
        "\n",
        "        return clf_lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_lgb = lbgclass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lgbreg(seed=42, data=data):\n",
        "        # Regresor\n",
        "        reg_lgb = lgb.LGBMRegressor(random_state=seed, n_jobs=-1, verbosity=-1)\n",
        "\n",
        "        # Agregamos la columna 'class' y dividimos en entrenamiento y prueba\n",
        "        data['class'] = clf_lgb.predict(data.drop(columns=['transactionRevenue']))\n",
        "        X = data.drop(columns=['transactionRevenue'])\n",
        "        y = data['transactionRevenue']\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "        # Creamos un random search para encontrar los mejores hiperparámetros\n",
        "        params = {\n",
        "                'boosting_type': ['gbdt', 'dart', 'goss'], # Tipo de modelo\n",
        "                'n_estimators': [100, 400, 800], # Número de árboles\n",
        "                'max_depth': [2, 3, 5], # Profundidad máxima\n",
        "                'learning_rate': [0.1, 0.06, 0.03, 0.01],  # Tasa de aprendizaje\n",
        "                }\n",
        "\n",
        "        # Obtenemos los mejores hiperparámetros\n",
        "        search = RandomizedSearchCV(\n",
        "                estimator=reg_lgb, \n",
        "                param_distributions=params, \n",
        "                scoring='neg_mean_squared_error', \n",
        "                cv=5, \n",
        "                n_iter=5)\n",
        "        search.fit(X_train, y_train)\n",
        "        best_params = search.best_params_\n",
        "\n",
        "        # Entrenamos el regresor\n",
        "        reg_lgb = lgb.LGBMRegressor(**best_params, random_state=seed, n_jobs=-1, verbosity=-1)\n",
        "        reg_lgb.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
        "\n",
        "        # Predecimos con el regresor\n",
        "        y_pred_reg_lgb = reg_lgb.predict(X_test)\n",
        "\n",
        "        # Convertimos todas las predicciones menores a 0.01 en 0\n",
        "        y_pred_reg_lgb = np.where(y_pred_reg_lgb < 0.01, 0, y_pred_reg_lgb)\n",
        "\n",
        "        # Score del regresor para rmse\n",
        "        print(f'Raiz del error para el regresor: {np.sqrt(mean_squared_error(y_test, y_pred_reg_lgb)):.3f}')\n",
        "\n",
        "        return reg_lgb, y_pred_reg_lgb, np.sqrt(mean_squared_error(y_test, y_pred_reg_lgb)), r2_score(y_test, y_pred_reg_lgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rmse = 21\n",
        "while rmse >= 16:\n",
        "    data = df.copy()\n",
        "    seed = np.random.randint(1, 100000)\n",
        "    reg_lgb, reg_lgb_pred, lgb_rmse, lgb_r2 = lgbreg(seed, data)\n",
        "    if rmse < 16:\n",
        "        print('El regresor logro el objetivo:')\n",
        "        print(f'El valor de la semilla es: {seed}')\n",
        "        print(f'Los hiperparámetros son: {reg_lgb.get_params()}')\n",
        "        print(f'El rmse es: {lgb_rmse}')\n",
        "        print(f'El r2 es: {lgb_r2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> mejores hiperparametros encontrados: `{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.06, 'max_depth': 5, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 17228, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'verbosity': -1}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Guardamos el modelo LGBM\n",
        "# dump(clf_lgb, 'clasificador_lgb.joblib')\n",
        "# dump(reg_lgb, 'regresor_lgb.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def catclass(seed=42, data=data):\n",
        "        # Clasificador\n",
        "        clf_cat = cat.CatBoostClassifier(random_state=seed, verbose=0)\n",
        "\n",
        "        # Dividimos en entrenamiento y prueba\n",
        "        X = data.drop('transactionRevenue', axis=1)\n",
        "        y = data['transactionRevenue'].apply(lambda x: 1 if x > 0 else 0)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "        # Creamos un random search para encontrar los mejores hiperparámetros\n",
        "        params = {\n",
        "                'learning_rate': [0.01, 0.05, 0.1, 0.3], # Tasa de aprendizaje\n",
        "                'depth': [2, 3, 5], # Profundidad máxima\n",
        "                'num_boost_round': [100, 400, 800] # Número de árboles\n",
        "                }\n",
        "\n",
        "        # Obtenemos los mejores hiperparámetros\n",
        "        search = RandomizedSearchCV(\n",
        "                estimator=clf_cat, \n",
        "                param_distributions=params, \n",
        "                scoring='accuracy', \n",
        "                cv=5, \n",
        "                n_iter=5)\n",
        "        search.fit(X_train, y_train)\n",
        "        best_params = search.best_params_\n",
        "\n",
        "        # Entrenamos el clasificador\n",
        "        clf_cat = cat.CatBoostClassifier(**best_params, verbose=0, random_state=seed)\n",
        "        clf_cat.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=20)\n",
        "\n",
        "        # Predecimos con el clasificador\n",
        "        y_pred_cls_lgb = clf_cat.predict(X_test)\n",
        "\n",
        "        # Score del clasificador\n",
        "        print(f'Precisión del clasificador: {accuracy_score(y_test, y_pred_cls_lgb):.3f}')\n",
        "\n",
        "        # Calibración más Precisa de las Probabilidades\n",
        "        cal_clf_cat = CalibratedClassifierCV(clf_cat, method='sigmoid')\n",
        "        cal_clf_cat.fit(X_train, y_train)\n",
        "\n",
        "        # Probabilidades de Clase Calibradas\n",
        "        calibrated_y_pred = cal_clf_cat.predict(X_test)\n",
        "\n",
        "        # Medir el Rendimiento\n",
        "        print(f'Precisión del clasificador calibrado: {accuracy_score(y_test, calibrated_y_pred):.3f}')\n",
        "\n",
        "        if accuracy_score(y_test, y_pred_cls_lgb) < accuracy_score(y_test, calibrated_y_pred):\n",
        "            clf_cat = cal_clf_cat\n",
        "\n",
        "        return clf_cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precisión del clasificador: 0.987\n",
            "Precisión del clasificador calibrado: 0.986\n"
          ]
        }
      ],
      "source": [
        "clf_cat = catclass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def catreg(seed=42, data=data):\n",
        "        # Regresor\n",
        "        reg_cat = cat.CatBoostRegressor(random_state=seed, verbose=0)\n",
        "\n",
        "        # Agregamos la columna 'class' y dividimos en entrenamiento y prueba\n",
        "        data['class'] = clf_cat.predict(data.drop(columns=['transactionRevenue']))\n",
        "        X = data.drop(columns=['transactionRevenue'])\n",
        "        y = data['transactionRevenue']\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "        # Creamos un random search para encontrar los mejores hiperparámetros\n",
        "        params = {\n",
        "                'learning_rate': [0.01, 0.05, 0.1, 0.3], # Tasa de aprendizaje\n",
        "                'depth': [2, 3, 5], # Profundidad máxima\n",
        "                'num_boost_round': [100, 400, 800] # Número de árboles\n",
        "                }\n",
        "\n",
        "        # Obtenemos los mejores hiperparámetros\n",
        "        search = RandomizedSearchCV(\n",
        "                estimator=reg_cat, \n",
        "                param_distributions=params, \n",
        "                scoring='neg_mean_squared_error', \n",
        "                cv=5, \n",
        "                n_iter=5)\n",
        "        search.fit(X_train, y_train)\n",
        "        best_params = search.best_params_\n",
        "\n",
        "        # Entrenamos el regresor\n",
        "        reg_cat = cat.CatBoostRegressor(**best_params, random_state=seed, verbose=0)\n",
        "        reg_cat.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=10, verbose=0)\n",
        "\n",
        "        # Predecimos con el regresor\n",
        "        y_pred_reg_cat = reg_cat.predict(X_test)\n",
        "\n",
        "        # Convertimos todas las predicciones menores a 0.01 en 0\n",
        "        y_pred_reg_cat = np.where(y_pred_reg_cat < 0.01, 0, y_pred_reg_cat)\n",
        "\n",
        "        # Score del regresor para rmse\n",
        "        print(f'Raiz del error para el regresor: {np.sqrt(mean_squared_error(y_test, y_pred_reg_cat)):.3f}')\n",
        "\n",
        "        return reg_cat, y_pred_reg_cat, np.sqrt(mean_squared_error(y_test, y_pred_reg_cat)), r2_score(y_test, y_pred_reg_cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raiz del error para el regresor: 17.597\n",
            "Raiz del error para el regresor: 17.407\n",
            "Raiz del error para el regresor: 17.506\n",
            "Raiz del error para el regresor: 17.501\n",
            "Raiz del error para el regresor: 17.352\n",
            "Raiz del error para el regresor: 17.502\n",
            "Raiz del error para el regresor: 17.509\n",
            "Raiz del error para el regresor: 17.595\n",
            "Raiz del error para el regresor: 17.253\n",
            "Raiz del error para el regresor: 17.468\n",
            "Raiz del error para el regresor: 17.496\n",
            "Raiz del error para el regresor: 17.264\n",
            "Raiz del error para el regresor: 17.817\n",
            "Raiz del error para el regresor: 17.611\n",
            "Raiz del error para el regresor: 17.913\n",
            "Raiz del error para el regresor: 17.443\n",
            "Raiz del error para el regresor: 17.489\n",
            "Raiz del error para el regresor: 17.467\n",
            "Raiz del error para el regresor: 17.803\n",
            "Raiz del error para el regresor: 17.385\n",
            "Raiz del error para el regresor: 17.450\n",
            "Raiz del error para el regresor: 17.629\n",
            "Raiz del error para el regresor: 17.484\n",
            "Raiz del error para el regresor: 17.572\n"
          ]
        }
      ],
      "source": [
        "rmse = 21\n",
        "while rmse >= 16:\n",
        "    data = df.copy()\n",
        "    seed = np.random.randint(1, 100000)\n",
        "    reg_cat, reg_cat_pred, cat_rmse, cat_r2 = catreg(seed, data)\n",
        "    if rmse < 16:\n",
        "        print('El regresor logro el objetivo:')\n",
        "        print(f'El valor de la semilla es: {seed}')\n",
        "        print(f'Los hiperparámetros son: {reg_cat.get_params()}')\n",
        "        print(f'El rmse es: {cat_rmse}')\n",
        "        print(f'El r2 es: {cat_r2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> mejores Hiperparametros encontrados: ``{'learning_rate': 0.01, 'depth': 3, 'loss_function': 'RMSE', 'verbose': 0, 'num_boost_round': 100, 'random_state': 28160}``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Guardamos el modelo CatBoost\n",
        "# dump(clf_cat, 'clasificador_cat.joblib')\n",
        "# dump(reg_cat, 'regresor_cat.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo de Votacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def votclass(data=data):    \n",
        "    # Dividimos en entrenamiento y prueba\n",
        "    X = data.drop('transactionRevenue', axis=1)\n",
        "    y = data['transactionRevenue'].apply(lambda x: 1 if x > 0 else 0)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Clasificador\n",
        "    clf = VotingClassifier(estimators=[('xgb', clf_xgb), ('lgb', clf_lgb), ('cat', clf_cat)], voting='soft', n_jobs=-1)\n",
        "\n",
        "    # Entrenamos el clasificador\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos con el clasificador\n",
        "    y_pred_cls = clf.predict(X_test)\n",
        "\n",
        "    # Score del clasificador\n",
        "    print(f'Precisión del clasificador: {accuracy_score(y_test, y_pred_cls):.3f}')\n",
        "\n",
        "    return clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = votclass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def votreg(data=data):\n",
        "    # Regresor\n",
        "    reg = VotingRegressor(estimators=[('xgb', reg_xgb), ('lgb', reg_lgb), ('cat', reg_cat)], n_jobs=-1)\n",
        "\n",
        "    # Agrergamos las columnas de clasificación\n",
        "    data['xgb_class'] = clf_xgb.predict(data.drop(columns=['transactionRevenue']))\n",
        "    data['lgb_class'] = clf_lgb.predict(data.drop(columns=['transactionRevenue', 'xgb_class']))\n",
        "    data['cat_class'] = clf_cat.predict(data.drop(columns=['transactionRevenue', 'xgb_class', 'lgb_class']))\n",
        "\n",
        "    X = data.drop(columns=['transactionRevenue'])\n",
        "    y = data['transactionRevenue']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Entrenamos el regresor\n",
        "    reg.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos con el regresor\n",
        "    y_pred_reg = reg.predict(X_test)\n",
        "\n",
        "    # Convertimos todas las predicciones menores a 0.01 en 0\n",
        "    y_pred_reg = np.where(y_pred_reg < 0.01, 0, y_pred_reg)\n",
        "\n",
        "    # Score del regresor para rmse\n",
        "    print(f'Raiz del error para el regresor: {np.sqrt(mean_squared_error(y_test, y_pred_reg)):.3f}')\n",
        "\n",
        "    return reg, y_pred_reg, np.sqrt(mean_squared_error(y_test, y_pred_reg)), r2_score(y_test, y_pred_reg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rmse = 21\n",
        "while rmse >= 16:\n",
        "    data = df.copy()\n",
        "    reg, y_pred_reg, vot_rmse, vot_r2 = votreg(data)\n",
        "    if rmse < 16:\n",
        "        print('El regresor logro el objetivo:')\n",
        "        print(f'Los hiperparámetros son: {reg.get_params()}')\n",
        "        print(f'El rmse es: {vot_rmse}')\n",
        "        print(f'El r2 es: {vot_r2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> mejores Hiperparametros encontrados: ``{'estimators': [('xgb', XGBRegressor(base_score=None, booster='gblinear', callbacks=None,\n",
        "             colsample_bylevel=None, colsample_bynode=None,\n",
        "             colsample_bytree=None, early_stopping_rounds=None,\n",
        "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
        "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
        "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
        "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
        "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
        "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
        "             n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
        "             predictor=None, random_state=31510, ...)), ('lgb', LGBMRegressor(boosting_type='dart', learning_rate=0.06, max_depth=5, n_jobs=-1,\n",
        "              random_state=17228, verbosity=-1)), ('cat', <catboost.core.CatBoostRegressor object at 0x000002630DE84F90>)], 'n_jobs': -1, 'verbose': False, 'weights': None, 'xgb': XGBRegressor(base_score=None, booster='gblinear', callbacks=None,\n",
        "             colsample_bylevel=None, colsample_bynode=None,\n",
        "             colsample_bytree=None, early_stopping_rounds=None,\n",
        "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
        "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
        "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
        "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
        "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
        "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
        "             n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
        "             predictor=None, random_state=31510, ...), 'lgb': LGBMRegressor(boosting_type='dart', learning_rate=0.06, max_depth=5, n_jobs=-1,\n",
        "              random_state=17228, verbosity=-1), 'cat': <catboost.core.CatBoostRegressor object at 0x000002630DE84F90>, 'xgb__objective': 'reg:squarederror', 'xgb__base_score': None, 'xgb__booster': 'gblinear', 'xgb__callbacks': None, 'xgb__colsample_bylevel': None, 'xgb__colsample_bynode': None, 'xgb__colsample_bytree': None, 'xgb__early_stopping_rounds': None, 'xgb__enable_categorical': False, 'xgb__eval_metric': None, 'xgb__feature_types': None, 'xgb__gamma': None, 'xgb__gpu_id': None, 'xgb__grow_policy': None, 'xgb__importance_type': None, 'xgb__interaction_constraints': None, 'xgb__learning_rate': 0.1, 'xgb__max_bin': None, 'xgb__max_cat_threshold': None, 'xgb__max_cat_to_onehot': None, 'xgb__max_delta_step': None, 'xgb__max_depth': None, 'xgb__max_leaves': None, 'xgb__min_child_weight': None, 'xgb__missing': nan, 'xgb__monotone_constraints': None, 'xgb__n_estimators': 500, 'xgb__n_jobs': None, 'xgb__num_parallel_tree': None, 'xgb__predictor': None, 'xgb__random_state': 31510, 'xgb__reg_alpha': None, 'xgb__reg_lambda': None, 'xgb__sampling_method': None, 'xgb__scale_pos_weight': None, 'xgb__subsample': None, 'xgb__tree_method': None, 'xgb__validate_parameters': None, 'xgb__verbosity': None, 'lgb__boosting_type': 'dart', 'lgb__class_weight': None, 'lgb__colsample_bytree': 1.0, 'lgb__importance_type': 'split', 'lgb__learning_rate': 0.06, 'lgb__max_depth': 5, 'lgb__min_child_samples': 20, 'lgb__min_child_weight': 0.001, 'lgb__min_split_gain': 0.0, 'lgb__n_estimators': 100, 'lgb__n_jobs': -1, 'lgb__num_leaves': 31, 'lgb__objective': None, 'lgb__random_state': 17228, 'lgb__reg_alpha': 0.0, 'lgb__reg_lambda': 0.0, 'lgb__subsample': 1.0, 'lgb__subsample_for_bin': 200000, 'lgb__subsample_freq': 0, 'lgb__verbosity': -1, 'cat__learning_rate': 0.01, 'cat__depth': 3, 'cat__loss_function': 'RMSE', 'cat__verbose': 0, 'cat__num_boost_round': 100, 'cat__random_state': 28160}``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Guardamos el modelo Votación\n",
        "# dump(clf, 'clasificador.joblib')\n",
        "# dump(reg, 'regresor.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y5Llo_mx9Cz"
      },
      "source": [
        "## **5. Evaluación y Selección del Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = data.drop(columns=['transactionRevenue'])\n",
        "y = data['transactionRevenue']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular la media\n",
        "mean_value = df['transactionRevenue'].mean()\n",
        "\n",
        "# Crear un array con la media repetida para cada observación\n",
        "mean_array = np.full(shape=df['transactionRevenue'].shape, fill_value=mean_value)\n",
        "\n",
        "# Calcular el MSE\n",
        "mse = mean_squared_error(df['transactionRevenue'], mean_array)\n",
        "\n",
        "print(f'La raiz del error cuadrático para la media: {np.sqrt(mse):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar modelos de Regresión gráficamente\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(range(len(y_test)), y_test, color='blue', label='Real', alpha=0.5)\n",
        "plt.scatter(range(len(reg_xgb_pred)), reg_xgb_pred, color='red', label='Predicho XGB', alpha=0.5)\n",
        "plt.scatter(range(len(reg_lgb_pred)), reg_lgb_pred, color='green', label='Predicho LGB', alpha=0.5)\n",
        "plt.scatter(range(len(reg_cat_pred)), reg_cat_pred, color='orange', label='Predicho CAT', alpha=0.5)\n",
        "plt.scatter(range(len(y_pred_reg)), y_pred_reg, color='purple', label='Predicho Votación', alpha=0.5)\n",
        "plt.title('Comparación de Modelos de Regresión')\n",
        "plt.xlabel('Índice')\n",
        "plt.ylabel('transactionRevenue')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar modelos de Regresion con el rmse gráficamente mediante barras\n",
        "rmse = [xgb_rmse, lgb_rmse, cat_rmse, vot_rmse]\n",
        "modelos = ['XGB', 'LGB', 'CAT', 'Votación']\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(modelos, rmse, color=['red', 'green', 'orange', 'purple'])\n",
        "plt.title('Comparación de Modelos de Regresión')\n",
        "plt.xlabel('Modelo')\n",
        "plt.ylabel('Raiz del Error')\n",
        "\n",
        "# Añadir el valor de cada barra en la gráfica\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), va='bottom')  # va: vertical alignment\n",
        "\n",
        "# R2 = 0\n",
        "plt.axhline(y=np.round(np.sqrt(mse), 2), color='r', linestyle='--')\n",
        "# Metrica de Alejo\n",
        "plt.axhline(y=16.76, color='b', linestyle='solid')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OYicz3XQPBWp",
        "RPkjyKUEWBIt",
        "l4GucSUdpzW9",
        "wRH2u-dX7WzM",
        "7y5Llo_mx9Cz",
        "jcSLZs21QeUk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
